\BOOKMARK [0][]{Title.0}{Titlepage}{}% 1
\BOOKMARK [0][]{acknowledgments.0}{Acknowledgments}{}% 2
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 3
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 4
\BOOKMARK [0][]{Resumo.0}{Resumo}{}% 5
\BOOKMARK [0][]{toc.0}{Contents}{}% 6
\BOOKMARK [1][]{lof.1}{List of Figures}{toc.0}% 7
\BOOKMARK [1][]{lot.1}{List of Tables}{toc.0}% 8
\BOOKMARK [1][]{loac.1}{Acronyms}{toc.0}% 9
\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 10
\BOOKMARK [1][]{section.1.1}{1.1 Context}{chapter.1}% 11
\BOOKMARK [1][]{section.1.2}{1.2 Problem statement}{chapter.1}% 12
\BOOKMARK [1][]{section.1.3}{1.3 Contributions}{chapter.1}% 13
\BOOKMARK [1][]{section.1.4}{1.4 Structure for the thesis}{chapter.1}% 14
\BOOKMARK [0][]{chapter.2}{2 Background - Children automatic speech recognition}{}% 15
\BOOKMARK [1][]{section.2.1}{2.1 Children speech recognition challenges}{chapter.2}% 16
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Speech variability}{section.2.1}% 17
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 Language and phonetic knowledge}{section.2.1}% 18
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 Data scarcity}{section.2.1}% 19
\BOOKMARK [1][]{section.2.2}{2.2 Introduction to automatic speech recognition}{chapter.2}% 20
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 A brief history of Automatic Speech Recognition}{section.2.2}% 21
\BOOKMARK [3][]{subsubsection.2.2.1.1}{2.2.1.A Early Days}{subsection.2.2.1}% 22
\BOOKMARK [3][]{subsubsection.2.2.1.2}{2.2.1.B The Speech Understanding Research program}{subsection.2.2.1}% 23
\BOOKMARK [2][]{subsection.2.2.2}{2.2.2 Traditional automatic speech recognition systems}{section.2.2}% 24
\BOOKMARK [3][]{subsubsection.2.2.2.1}{2.2.2.A Feature extraction}{subsection.2.2.2}% 25
\BOOKMARK [3][]{subsubsection.2.2.2.2}{2.2.2.B Acoustic model}{subsection.2.2.2}% 26
\BOOKMARK [3][]{subsubsection.2.2.2.3}{2.2.2.C Pronunciation model}{subsection.2.2.2}% 27
\BOOKMARK [3][]{subsubsection.2.2.2.4}{2.2.2.D Language model}{subsection.2.2.2}% 28
\BOOKMARK [3][]{subsubsection.2.2.2.5}{2.2.2.E Decoder}{subsection.2.2.2}% 29
\BOOKMARK [2][]{subsection.2.2.3}{2.2.3 End-to-end automatic speech recognition}{section.2.2}% 30
\BOOKMARK [3][]{subsubsection.2.2.3.1}{2.2.3.A Connectionist Temporal Classification}{subsection.2.2.3}% 31
\BOOKMARK [3][]{subsubsection.2.2.3.2}{2.2.3.B Sequence to sequence}{subsection.2.2.3}% 32
\BOOKMARK [2][]{subsection.2.2.4}{2.2.4 Automatic Speech Recognition metrics}{section.2.2}% 33
\BOOKMARK [1][]{section.2.3}{2.3 Children automatic speech recognition}{chapter.2}% 34
\BOOKMARK [2][]{subsection.2.3.1}{2.3.1 Feature extraction stage}{section.2.3}% 35
\BOOKMARK [2][]{subsection.2.3.2}{2.3.2 Pronunciation and language model}{section.2.3}% 36
\BOOKMARK [2][]{subsection.2.3.3}{2.3.3 Design of acoustic models}{section.2.3}% 37
\BOOKMARK [2][]{subsection.2.3.4}{2.3.4 End-to-end models}{section.2.3}% 38
\BOOKMARK [2][]{subsection.2.3.5}{2.3.5 Data augmentation}{section.2.3}% 39
\BOOKMARK [3][]{subsubsection.2.3.5.1}{2.3.5.A Using external data}{subsection.2.3.5}% 40
\BOOKMARK [3][]{subsubsection.2.3.5.2}{2.3.5.B Using available data}{subsection.2.3.5}% 41
\BOOKMARK [2][]{subsection.2.3.6}{2.3.6 Training procedure for children speech recognition}{section.2.3}% 42
\BOOKMARK [3][]{subsubsection.2.3.6.1}{2.3.6.A Transfer learning}{subsection.2.3.6}% 43
\BOOKMARK [3][]{subsubsection.2.3.6.2}{2.3.6.B Multi-task learning}{subsection.2.3.6}% 44
\BOOKMARK [3][]{subsubsection.2.3.6.3}{2.3.6.C Self-supervised Learning}{subsection.2.3.6}% 45
\BOOKMARK [1][]{section.2.4}{2.4 Children Corpora}{chapter.2}% 46
\BOOKMARK [2][]{subsection.2.4.1}{2.4.1 LETSREAD}{section.2.4}% 47
\BOOKMARK [2][]{subsection.2.4.2}{2.4.2 PFSTAR\137SWEDISH}{section.2.4}% 48
\BOOKMARK [2][]{subsection.2.4.3}{2.4.3 ETLTDE}{section.2.4}% 49
\BOOKMARK [2][]{subsection.2.4.4}{2.4.4 CMU\137KIDS}{section.2.4}% 50
\BOOKMARK [2][]{subsection.2.4.5}{2.4.5 CHOREC}{section.2.4}% 51
\BOOKMARK [2][]{subsection.2.4.6}{2.4.6 MyST}{section.2.4}% 52
\BOOKMARK [1][]{section.2.5}{2.5 Summary}{chapter.2}% 53
\BOOKMARK [0][]{chapter.3}{3 Hybrid models for children automatic speech recognition}{}% 54
\BOOKMARK [1][]{section.3.1}{3.1 Introduction}{chapter.3}% 55
\BOOKMARK [1][]{section.3.2}{3.2 Factorised Time Delay Neural Network for children ASR}{chapter.3}% 56
\BOOKMARK [1][]{section.3.3}{3.3 Assessing the efficacy of multi-task and transfer learning from adult to children}{chapter.3}% 57
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Methodology}{section.3.3}% 58
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 Corpus}{section.3.3}% 59
\BOOKMARK [2][]{subsection.3.3.3}{3.3.3 Experimental setup}{section.3.3}% 60
\BOOKMARK [2][]{subsection.3.3.4}{3.3.4 Results}{section.3.3}% 61
\BOOKMARK [2][]{subsection.3.3.5}{3.3.5 Summary and discussion}{section.3.3}% 62
\BOOKMARK [1][]{section.3.4}{3.4 Combining multi-task and transfer learning using multilingual children data}{chapter.3}% 63
\BOOKMARK [2][]{subsection.3.4.1}{3.4.1 Motivation}{section.3.4}% 64
\BOOKMARK [2][]{subsection.3.4.2}{3.4.2 The Multilingual-transfer learning approach}{section.3.4}% 65
\BOOKMARK [2][]{subsection.3.4.3}{3.4.3 Experimental Setup}{section.3.4}% 66
\BOOKMARK [2][]{subsection.3.4.4}{3.4.4 Multilingual-transfer learning experiment}{section.3.4}% 67
\BOOKMARK [2][]{subsection.3.4.5}{3.4.5 Cross-lingual validation}{section.3.4}% 68
\BOOKMARK [1][]{section.3.5}{3.5 Summary and discussion}{chapter.3}% 69
\BOOKMARK [0][]{chapter.4}{4 End-to-End children automatic speech recognition}{}% 70
\BOOKMARK [1][]{section.4.1}{4.1 Introduction}{chapter.4}% 71
\BOOKMARK [1][]{section.4.2}{4.2 Transformer model}{chapter.4}% 72
\BOOKMARK [1][]{section.4.3}{4.3 Conformer model}{chapter.4}% 73
\BOOKMARK [1][]{section.4.4}{4.4 Understand transfer learning efficacy for Transformer based models}{chapter.4}% 74
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Partial Transfer learning}{section.4.4}% 75
\BOOKMARK [2][]{subsection.4.4.2}{4.4.2 Experimental setup}{section.4.4}% 76
\BOOKMARK [2][]{subsection.4.4.3}{4.4.3 Corpus}{section.4.4}% 77
\BOOKMARK [2][]{subsection.4.4.4}{4.4.4 Implementation details}{section.4.4}% 78
\BOOKMARK [2][]{subsection.4.4.5}{4.4.5 Encoder-Decoder Transfer learning}{section.4.4}% 79
\BOOKMARK [2][]{subsection.4.4.6}{4.4.6 Modules Transfer learning}{section.4.4}% 80
\BOOKMARK [1][]{section.4.5}{4.5 Summary and discussion}{chapter.4}% 81
\BOOKMARK [0][]{chapter.5}{5 Exploring Parameter-Efficient Strategies in Transfer Learning for Children-Focused ASR Systems}{}% 82
\BOOKMARK [1][]{section.5.1}{5.1 Introduction}{chapter.5}% 83
\BOOKMARK [1][]{section.5.2}{5.2 Adapter tuning}{chapter.5}% 84
\BOOKMARK [1][]{section.5.3}{5.3 Investigating Adapters for Children's ASR}{chapter.5}% 85
\BOOKMARK [1][]{section.5.4}{5.4 Implementation details}{chapter.5}% 86
\BOOKMARK [1][]{section.5.5}{5.5 Results}{chapter.5}% 87
\BOOKMARK [2][]{subsection.5.5.1}{5.5.1 Configurations}{section.5.5}% 88
\BOOKMARK [2][]{subsection.5.5.2}{5.5.2 Effect of the Adapter hidden dimension}{section.5.5}% 89
\BOOKMARK [2][]{subsection.5.5.3}{5.5.3 Unsupervised clustering for grouped-speaker Adapters}{section.5.5}% 90
\BOOKMARK [1][]{section.5.6}{5.6 Summary and discussion}{chapter.5}% 91
\BOOKMARK [0][]{chapter.6}{6 Integration of synthetic speech for data augmentation}{}% 92
\BOOKMARK [1][]{section.6.1}{6.1 Introduction}{chapter.6}% 93
\BOOKMARK [1][]{section.6.2}{6.2 Enhancing ASR Performance through TTS Data Augmentation}{chapter.6}% 94
\BOOKMARK [1][]{section.6.3}{6.3 Closing the synthetic and real mismatch gap with Adapters}{chapter.6}% 95
\BOOKMARK [1][]{section.6.4}{6.4 Overview of the automatic speech recognition and text-to-speechs systems}{chapter.6}% 96
\BOOKMARK [2][]{subsection.6.4.1}{6.4.1 Transformer architecture for ASR}{section.6.4}% 97
\BOOKMARK [2][]{subsection.6.4.2}{6.4.2 Multi-speaker text-to-speech: YourTTS}{section.6.4}% 98
\BOOKMARK [1][]{section.6.5}{6.5 Experimental setup}{chapter.6}% 99
\BOOKMARK [2][]{subsection.6.5.1}{6.5.1 Real speech corpus}{section.6.5}% 100
\BOOKMARK [2][]{subsection.6.5.2}{6.5.2 Synthetic data}{section.6.5}% 101
\BOOKMARK [2][]{subsection.6.5.3}{6.5.3 Experiments}{section.6.5}% 102
\BOOKMARK [1][]{section.6.6}{6.6 Results and discussion}{chapter.6}% 103
\BOOKMARK [2][]{subsection.6.6.1}{6.6.1 Comparison with existing approaches}{section.6.6}% 104
\BOOKMARK [2][]{subsection.6.6.2}{6.6.2 Influence of synthetic number of hours}{section.6.6}% 105
\BOOKMARK [2][]{subsection.6.6.3}{6.6.3 Impact of DWAT different hyper-parameters}{section.6.6}% 106
\BOOKMARK [2][]{subsection.6.6.4}{6.6.4 Extension DWAT to the Conformer architecture}{section.6.6}% 107
\BOOKMARK [1][]{section.6.7}{6.7 Summary and discussion}{chapter.6}% 108
\BOOKMARK [0][]{chapter.7}{7 Alternative approaches to parameter-efficient transfer learning}{}% 109
\BOOKMARK [1][]{section.7.1}{7.1 Introduction}{chapter.7}% 110
\BOOKMARK [1][]{section.7.2}{7.2 Exploring PETL literature alternatives}{chapter.7}% 111
\BOOKMARK [2][]{subsection.7.2.1}{7.2.1 Scaled Adapters}{section.7.2}% 112
\BOOKMARK [2][]{subsection.7.2.2}{7.2.2 Convolution based Adapters}{section.7.2}% 113
\BOOKMARK [2][]{subsection.7.2.3}{7.2.3 BitFit}{section.7.2}% 114
\BOOKMARK [2][]{subsection.7.2.4}{7.2.4 Scale and Shift features}{section.7.2}% 115
\BOOKMARK [2][]{subsection.7.2.5}{7.2.5 AdapterBias}{section.7.2}% 116
\BOOKMARK [2][]{subsection.7.2.6}{7.2.6 Results of the different PETL methods}{section.7.2}% 117
\BOOKMARK [1][]{section.7.3}{7.3 Advancement in Adapters: Introducing Shared-Adapters}{chapter.7}% 118
\BOOKMARK [2][]{subsection.7.3.1}{7.3.1 Motivation}{section.7.3}% 119
\BOOKMARK [2][]{subsection.7.3.2}{7.3.2 Experimental setup}{section.7.3}% 120
\BOOKMARK [2][]{subsection.7.3.3}{7.3.3 Results}{section.7.3}% 121
\BOOKMARK [1][]{section.7.4}{7.4 Summary and discussion}{chapter.7}% 122
\BOOKMARK [0][]{chapter.8}{8 Conclusions}{}% 123
\BOOKMARK [1][]{section.8.1}{8.1 Summary of the work carried out during the thesis}{chapter.8}% 124
\BOOKMARK [1][]{section.8.2}{8.2 Perspectives}{chapter.8}% 125
\BOOKMARK [0][]{bib.0}{Bibliography}{}% 126
\BOOKMARK [0][]{section*.125}{Bibliography}{}% 127
\BOOKMARK [1][]{appendix.1}{Appendix A}{section*.125}% 128
\BOOKMARK [0][]{appendix.A}{A Pathological speech detection through pre-trained models}{}% 129
\BOOKMARK [1][]{section.A.1}{A.1 Introduction}{appendix.A}% 130
\BOOKMARK [1][]{section.A.2}{A.2 Pathological speech detection using x-vector embeddings}{appendix.A}% 131
\BOOKMARK [2][]{subsection.A.2.1}{A.2.1 Introduction}{section.A.2}% 132
\BOOKMARK [2][]{subsection.A.2.2}{A.2.2 Speaker embeddings: i-vector and x-vector}{section.A.2}% 133
\BOOKMARK [2][]{subsection.A.2.3}{A.2.3 Experimental setup}{section.A.2}% 134
\BOOKMARK [3][]{subsubsection.A.2.3.1}{A.2.3.A Corpora}{subsection.A.2.3}% 135
\BOOKMARK [3][]{subsubsection.A.2.3.2}{A.2.3.B Knowledge based features}{subsection.A.2.3}% 136
\BOOKMARK [3][]{subsubsection.A.2.3.3}{A.2.3.C Speaker embeddings}{subsection.A.2.3}% 137
\BOOKMARK [2][]{subsection.A.2.4}{A.2.4 Results}{section.A.2}% 138
\BOOKMARK [1][]{section.A.3}{A.3 The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge}{appendix.A}% 139
\BOOKMARK [2][]{subsection.A.3.1}{A.3.1 Introduction}{section.A.3}% 140
\BOOKMARK [2][]{subsection.A.3.2}{A.3.2 Corpus}{section.A.3}% 141
\BOOKMARK [2][]{subsection.A.3.3}{A.3.3 Proposed system}{section.A.3}% 142
\BOOKMARK [3][]{subsubsection.A.3.3.1}{A.3.3.A Acoustics modality}{subsection.A.3.3}% 143
\BOOKMARK [3][]{subsubsection.A.3.3.2}{A.3.3.B Linguistic modality}{subsection.A.3.3}% 144
\BOOKMARK [2][]{subsection.A.3.4}{A.3.4 Results}{section.A.3}% 145
\BOOKMARK [1][]{section.A.4}{A.4 Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19}{appendix.A}% 146
\BOOKMARK [2][]{subsection.A.4.1}{A.4.1 Introduction}{section.A.4}% 147
\BOOKMARK [2][]{subsection.A.4.2}{A.4.2 Corpora}{section.A.4}% 148
\BOOKMARK [2][]{subsection.A.4.3}{A.4.3 Proposed system}{section.A.4}% 149
\BOOKMARK [3][]{subsubsection.A.4.3.1}{A.4.3.A TDNN-F embedddings}{subsection.A.4.3}% 150
\BOOKMARK [3][]{subsubsection.A.4.3.2}{A.4.3.B CNN embedddings}{subsection.A.4.3}% 151
\BOOKMARK [3][]{subsubsection.A.4.3.3}{A.4.3.C PASE+ embedddings}{subsection.A.4.3}% 152
\BOOKMARK [3][]{subsubsection.A.4.3.4}{A.4.3.D COVID-19 condition classification}{subsection.A.4.3}% 153
\BOOKMARK [2][]{subsection.A.4.4}{A.4.4 Results}{section.A.4}% 154
\BOOKMARK [1][]{section.A.5}{A.5 Conclusion and future work}{appendix.A}% 155
\BOOKMARK [1][]{appendix.1}{Appendix B}{appendix.A}% 156
\BOOKMARK [0][]{appendix.B}{B Sefl-supervised learning as feature extractor for children's ASR}{}% 157
\BOOKMARK [1][]{section.B.1}{B.1 Introduction}{appendix.B}% 158
\BOOKMARK [1][]{section.B.2}{B.2 Self-supervised pre-trained models}{appendix.B}% 159
\BOOKMARK [2][]{subsection.B.2.1}{B.2.1 Generative modeling}{section.B.2}% 160
\BOOKMARK [2][]{subsection.B.2.2}{B.2.2 Discriminative modeling}{section.B.2}% 161
\BOOKMARK [1][]{section.B.3}{B.3 Experimental setup}{appendix.B}% 162
\BOOKMARK [1][]{section.B.4}{B.4 Results}{appendix.B}% 163
\BOOKMARK [1][]{section.B.5}{B.5 Analysis of the extracted features}{appendix.B}% 164
\BOOKMARK [1][]{section.B.6}{B.6 Conclusions and future work}{appendix.B}% 165
