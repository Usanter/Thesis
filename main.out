\BOOKMARK [0][]{Title.0}{Titlepage}{}% 1
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 2
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 3
\BOOKMARK [0][]{toc.0}{Contents}{}% 4
\BOOKMARK [1][]{lof.1}{List of Figures}{toc.0}% 5
\BOOKMARK [1][]{lot.1}{List of Tables}{toc.0}% 6
\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 7
\BOOKMARK [1][]{section.1.1}{1.1 Context}{chapter.1}% 8
\BOOKMARK [1][]{section.1.2}{1.2 Problem statement}{chapter.1}% 9
\BOOKMARK [1][]{section.1.3}{1.3 Contributions}{chapter.1}% 10
\BOOKMARK [1][]{section.1.4}{1.4 Structure for the thesis}{chapter.1}% 11
\BOOKMARK [0][]{chapter.2}{2 Background - Children automatic speech recognition}{}% 12
\BOOKMARK [1][]{section.2.1}{2.1 Children speech recognition challenges}{chapter.2}% 13
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Speech variability}{section.2.1}% 14
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 Language and phonetic knowledge}{section.2.1}% 15
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 Data scarcity}{section.2.1}% 16
\BOOKMARK [1][]{section.2.2}{2.2 Introduction to automatic speech recognition}{chapter.2}% 17
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 A brief history of Automatic Speech Recognition}{section.2.2}% 18
\BOOKMARK [3][]{subsubsection.2.2.1.1}{2.2.1.A Early Days}{subsection.2.2.1}% 19
\BOOKMARK [3][]{subsubsection.2.2.1.2}{2.2.1.B The Speech Understanding Research program}{subsection.2.2.1}% 20
\BOOKMARK [2][]{subsection.2.2.2}{2.2.2 Traditional automatic speech recognition systems}{section.2.2}% 21
\BOOKMARK [3][]{subsubsection.2.2.2.1}{2.2.2.A Feature extraction}{subsection.2.2.2}% 22
\BOOKMARK [3][]{subsubsection.2.2.2.2}{2.2.2.B Acoustic model}{subsection.2.2.2}% 23
\BOOKMARK [3][]{subsubsection.2.2.2.3}{2.2.2.C Pronunciation model}{subsection.2.2.2}% 24
\BOOKMARK [3][]{subsubsection.2.2.2.4}{2.2.2.D Language model}{subsection.2.2.2}% 25
\BOOKMARK [3][]{subsubsection.2.2.2.5}{2.2.2.E Decoder}{subsection.2.2.2}% 26
\BOOKMARK [2][]{subsection.2.2.3}{2.2.3 End-to-end automatic speech recognition}{section.2.2}% 27
\BOOKMARK [3][]{subsubsection.2.2.3.1}{2.2.3.A Connectionist Temporal Classification}{subsection.2.2.3}% 28
\BOOKMARK [3][]{subsubsection.2.2.3.2}{2.2.3.B Sequence to sequence}{subsection.2.2.3}% 29
\BOOKMARK [2][]{subsection.2.2.4}{2.2.4 Automatic Speech Recognition metrics}{section.2.2}% 30
\BOOKMARK [1][]{section.2.3}{2.3 Children automatic speech recognition}{chapter.2}% 31
\BOOKMARK [2][]{subsection.2.3.1}{2.3.1 Feature extraction stage}{section.2.3}% 32
\BOOKMARK [2][]{subsection.2.3.2}{2.3.2 Pronunciation and language model}{section.2.3}% 33
\BOOKMARK [2][]{subsection.2.3.3}{2.3.3 Design of acoustic models}{section.2.3}% 34
\BOOKMARK [2][]{subsection.2.3.4}{2.3.4 End-to-end models}{section.2.3}% 35
\BOOKMARK [2][]{subsection.2.3.5}{2.3.5 Data augmentation}{section.2.3}% 36
\BOOKMARK [3][]{subsubsection.2.3.5.1}{2.3.5.A Using external data}{subsection.2.3.5}% 37
\BOOKMARK [3][]{subsubsection.2.3.5.2}{2.3.5.B Using available data}{subsection.2.3.5}% 38
\BOOKMARK [2][]{subsection.2.3.6}{2.3.6 Training procedure for children speech recognition}{section.2.3}% 39
\BOOKMARK [3][]{subsubsection.2.3.6.1}{2.3.6.A Transfer learning}{subsection.2.3.6}% 40
\BOOKMARK [3][]{subsubsection.2.3.6.2}{2.3.6.B Multi-task learning}{subsection.2.3.6}% 41
\BOOKMARK [3][]{subsubsection.2.3.6.3}{2.3.6.C Self-supervised Learning}{subsection.2.3.6}% 42
\BOOKMARK [1][]{section.2.4}{2.4 Children Corpora}{chapter.2}% 43
\BOOKMARK [2][]{subsection.2.4.1}{2.4.1 LETSREAD}{section.2.4}% 44
\BOOKMARK [2][]{subsection.2.4.2}{2.4.2 PFSTAR\137SWEDISH}{section.2.4}% 45
\BOOKMARK [2][]{subsection.2.4.3}{2.4.3 ETLTDE}{section.2.4}% 46
\BOOKMARK [2][]{subsection.2.4.4}{2.4.4 CMU\137KIDS}{section.2.4}% 47
\BOOKMARK [2][]{subsection.2.4.5}{2.4.5 CHOREC}{section.2.4}% 48
\BOOKMARK [2][]{subsection.2.4.6}{2.4.6 MyST}{section.2.4}% 49
\BOOKMARK [1][]{section.2.5}{2.5 Summary}{chapter.2}% 50
\BOOKMARK [0][]{chapter.3}{3 Hybrid models for children automatic speech recognition}{}% 51
\BOOKMARK [1][]{section.3.1}{3.1 Introduction}{chapter.3}% 52
\BOOKMARK [1][]{section.3.2}{3.2 Multi-task and Transfer learning using adult and children data}{chapter.3}% 53
\BOOKMARK [2][]{subsection.3.2.1}{3.2.1 Methodology}{section.3.2}% 54
\BOOKMARK [2][]{subsection.3.2.2}{3.2.2 Corpus}{section.3.2}% 55
\BOOKMARK [2][]{subsection.3.2.3}{3.2.3 Experimental setup}{section.3.2}% 56
\BOOKMARK [2][]{subsection.3.2.4}{3.2.4 Results}{section.3.2}% 57
\BOOKMARK [2][]{subsection.3.2.5}{3.2.5 Summary and discussion}{section.3.2}% 58
\BOOKMARK [1][]{section.3.3}{3.3 Multi-task and transfer learning using multilingual children data}{chapter.3}% 59
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Motivation}{section.3.3}% 60
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 Proposed approach}{section.3.3}% 61
\BOOKMARK [2][]{subsection.3.3.3}{3.3.3 Setup}{section.3.3}% 62
\BOOKMARK [2][]{subsection.3.3.4}{3.3.4 Multilingual-transfer learning experiment}{section.3.3}% 63
\BOOKMARK [2][]{subsection.3.3.5}{3.3.5 Cross-lingual validation}{section.3.3}% 64
\BOOKMARK [2][]{subsection.3.3.6}{3.3.6 Summary and discussion}{section.3.3}% 65
\BOOKMARK [1][]{section.3.4}{3.4 Conclusion}{chapter.3}% 66
\BOOKMARK [0][]{chapter.4}{4 End-to-End children automatic speech recognition}{}% 67
\BOOKMARK [1][]{section.4.1}{4.1 Introduction}{chapter.4}% 68
\BOOKMARK [1][]{section.4.2}{4.2 Transformer model}{chapter.4}% 69
\BOOKMARK [1][]{section.4.3}{4.3 Conformer model}{chapter.4}% 70
\BOOKMARK [1][]{section.4.4}{4.4 Understand transfer learning efficacy for transformer based models}{chapter.4}% 71
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Partial Transfer learning}{section.4.4}% 72
\BOOKMARK [2][]{subsection.4.4.2}{4.4.2 Experimental setup}{section.4.4}% 73
\BOOKMARK [2][]{subsection.4.4.3}{4.4.3 Corpus}{section.4.4}% 74
\BOOKMARK [2][]{subsection.4.4.4}{4.4.4 Implementation details}{section.4.4}% 75
\BOOKMARK [2][]{subsection.4.4.5}{4.4.5 Encoder-Decoder Transfer learning}{section.4.4}% 76
\BOOKMARK [2][]{subsection.4.4.6}{4.4.6 Modules Transfer learning}{section.4.4}% 77
\BOOKMARK [1][]{section.4.5}{4.5 Summary}{chapter.4}% 78
\BOOKMARK [0][]{chapter.5}{5 Exploring parameters-efficient transfer learning for end-to-end children ASR}{}% 79
\BOOKMARK [1][]{section.5.1}{5.1 Introduction}{chapter.5}% 80
\BOOKMARK [1][]{section.5.2}{5.2 Adapters}{chapter.5}% 81
\BOOKMARK [1][]{section.5.3}{5.3 Investigating Adapters for children ASR}{chapter.5}% 82
\BOOKMARK [1][]{section.5.4}{5.4 Implementation details}{chapter.5}% 83
\BOOKMARK [1][]{section.5.5}{5.5 Results}{chapter.5}% 84
\BOOKMARK [2][]{subsection.5.5.1}{5.5.1 Configurations}{section.5.5}% 85
\BOOKMARK [2][]{subsection.5.5.2}{5.5.2 Unsupervised Clustering of utterances}{section.5.5}% 86
\BOOKMARK [1][]{section.5.6}{5.6 Exploring PETL alternative approaches}{chapter.5}% 87
\BOOKMARK [2][]{subsection.5.6.1}{5.6.1 Scaled Adapters}{section.5.6}% 88
\BOOKMARK [2][]{subsection.5.6.2}{5.6.2 Convolution based Adapters}{section.5.6}% 89
\BOOKMARK [2][]{subsection.5.6.3}{5.6.3 BitFit}{section.5.6}% 90
\BOOKMARK [2][]{subsection.5.6.4}{5.6.4 Scale and Shift features}{section.5.6}% 91
\BOOKMARK [2][]{subsection.5.6.5}{5.6.5 AdapterBias}{section.5.6}% 92
\BOOKMARK [1][]{section.5.7}{5.7 Results of the different PETL methods}{chapter.5}% 93
\BOOKMARK [1][]{section.5.8}{5.8 Shared Adapter}{chapter.5}% 94
\BOOKMARK [2][]{subsection.5.8.1}{5.8.1 Experimental setup}{section.5.8}% 95
\BOOKMARK [2][]{subsection.5.8.2}{5.8.2 Results}{section.5.8}% 96
\BOOKMARK [0][]{chapter.6}{6 Use of synthetic speech as data augmentation}{}% 97
\BOOKMARK [1][]{section.6.1}{6.1 Introduction}{chapter.6}% 98
\BOOKMARK [1][]{section.6.2}{6.2 Related work}{chapter.6}% 99
\BOOKMARK [2][]{subsection.6.2.1}{6.2.1 TTS data augmentation}{section.6.2}% 100
\BOOKMARK [2][]{subsection.6.2.2}{6.2.2 Adapters}{section.6.2}% 101
\BOOKMARK [1][]{section.6.3}{6.3 Method}{chapter.6}% 102
\BOOKMARK [1][]{section.6.4}{6.4 System description}{chapter.6}% 103
\BOOKMARK [2][]{subsection.6.4.1}{6.4.1 Transformer architecture for ASR}{section.6.4}% 104
\BOOKMARK [2][]{subsection.6.4.2}{6.4.2 Multi-speaker text-to-speech: YourTTS}{section.6.4}% 105
\BOOKMARK [1][]{section.6.5}{6.5 Experimental setup}{chapter.6}% 106
\BOOKMARK [2][]{subsection.6.5.1}{6.5.1 Real speech corpus}{section.6.5}% 107
\BOOKMARK [2][]{subsection.6.5.2}{6.5.2 Synthetic data}{section.6.5}% 108
\BOOKMARK [2][]{subsection.6.5.3}{6.5.3 Experiments}{section.6.5}% 109
\BOOKMARK [1][]{section.6.6}{6.6 Results and discussion}{chapter.6}% 110
\BOOKMARK [2][]{subsection.6.6.1}{6.6.1 Comparison with existing approaches}{section.6.6}% 111
\BOOKMARK [2][]{subsection.6.6.2}{6.6.2 Effect of the number of hours}{section.6.6}% 112
\BOOKMARK [2][]{subsection.6.6.3}{6.6.3 Effect of the Adapters hyper-parameters}{section.6.6}% 113
\BOOKMARK [1][]{section.6.7}{6.7 Conclusions and future work}{chapter.6}% 114
\BOOKMARK [1][]{section.6.8}{6.8 Ongoing and future work}{chapter.6}% 115
\BOOKMARK [0][]{chapter.7}{7 Pathology detection from speech}{}% 116
\BOOKMARK [0][]{bib.0}{Bibliography}{}% 117
\BOOKMARK [0][]{chapter.7}{Bibliography}{}% 118
\BOOKMARK [1][]{appendix.1}{Appendix A}{chapter.7}% 119
\BOOKMARK [0][]{appendix.A}{A Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19}{}% 120
\BOOKMARK [1][]{section.A.1}{A.1 Introduction}{appendix.A}% 121
\BOOKMARK [1][]{appendix.1}{Appendix B}{appendix.A}% 122
\BOOKMARK [0][]{appendix.B}{B Sefl-supervised learning as feature extractor for children's ASR}{}% 123
\BOOKMARK [1][]{section.B.1}{B.1 Introduction}{appendix.B}% 124
\BOOKMARK [1][]{section.B.2}{B.2 Generative modeling}{appendix.B}% 125
\BOOKMARK [1][]{section.B.3}{B.3 Discriminative modeling}{appendix.B}% 126
\BOOKMARK [1][]{section.B.4}{B.4 Wav2Vec2}{appendix.B}% 127
\BOOKMARK [1][]{section.B.5}{B.5 HuBert}{appendix.B}% 128
\BOOKMARK [1][]{section.B.6}{B.6 Experimental setup}{appendix.B}% 129
\BOOKMARK [1][]{section.B.7}{B.7 Results}{appendix.B}% 130
