\BOOKMARK [0][]{Title.0}{Titlepage}{}% 1
\BOOKMARK [0][]{acknowledgments.0}{Acknowledgments}{}% 2
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 3
\BOOKMARK [0][]{Abstract.0}{Abstract}{}% 4
\BOOKMARK [0][]{Resumo.0}{Resumo}{}% 5
\BOOKMARK [0][]{toc.0}{Contents}{}% 6
\BOOKMARK [1][]{lof.1}{List of Figures}{toc.0}% 7
\BOOKMARK [1][]{lot.1}{List of Tables}{toc.0}% 8
\BOOKMARK [1][]{loac.1}{Acronyms}{toc.0}% 9
\BOOKMARK [0][]{chapter.1}{1 Introduction}{}% 10
\BOOKMARK [1][]{section.1.1}{1.1 Problem statement}{chapter.1}% 11
\BOOKMARK [1][]{section.1.2}{1.2 Contributions}{chapter.1}% 12
\BOOKMARK [1][]{section.1.3}{1.3 Structure for the thesis}{chapter.1}% 13
\BOOKMARK [0][]{chapter.2}{2 Background - Children automatic speech recognition}{}% 14
\BOOKMARK [1][]{section.2.1}{2.1 Children speech recognition challenges}{chapter.2}% 15
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Speech variability}{section.2.1}% 16
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 Language and phonetic knowledge}{section.2.1}% 17
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 Data scarcity}{section.2.1}% 18
\BOOKMARK [1][]{section.2.2}{2.2 Introduction to automatic speech recognition}{chapter.2}% 19
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 A brief history of Automatic Speech Recognition}{section.2.2}% 20
\BOOKMARK [3][]{subsubsection.2.2.1.1}{2.2.1.A Early Days}{subsection.2.2.1}% 21
\BOOKMARK [3][]{subsubsection.2.2.1.2}{2.2.1.B The Speech Understanding Research program}{subsection.2.2.1}% 22
\BOOKMARK [2][]{subsection.2.2.2}{2.2.2 Traditional automatic speech recognition systems}{section.2.2}% 23
\BOOKMARK [3][]{subsubsection.2.2.2.1}{2.2.2.A Feature extraction}{subsection.2.2.2}% 24
\BOOKMARK [3][]{subsubsection.2.2.2.2}{2.2.2.B Acoustic model}{subsection.2.2.2}% 25
\BOOKMARK [3][]{subsubsection.2.2.2.3}{2.2.2.C Pronunciation model}{subsection.2.2.2}% 26
\BOOKMARK [3][]{subsubsection.2.2.2.4}{2.2.2.D Language model}{subsection.2.2.2}% 27
\BOOKMARK [3][]{subsubsection.2.2.2.5}{2.2.2.E Decoder}{subsection.2.2.2}% 28
\BOOKMARK [2][]{subsection.2.2.3}{2.2.3 End-to-end automatic speech recognition}{section.2.2}% 29
\BOOKMARK [3][]{subsubsection.2.2.3.1}{2.2.3.A Connectionist Temporal Classification}{subsection.2.2.3}% 30
\BOOKMARK [3][]{subsubsection.2.2.3.2}{2.2.3.B Sequence to sequence}{subsection.2.2.3}% 31
\BOOKMARK [2][]{subsection.2.2.4}{2.2.4 Automatic Speech Recognition metrics}{section.2.2}% 32
\BOOKMARK [1][]{section.2.3}{2.3 Children automatic speech recognition}{chapter.2}% 33
\BOOKMARK [2][]{subsection.2.3.1}{2.3.1 Feature extraction stage}{section.2.3}% 34
\BOOKMARK [2][]{subsection.2.3.2}{2.3.2 Pronunciation and language model}{section.2.3}% 35
\BOOKMARK [2][]{subsection.2.3.3}{2.3.3 Design of acoustic models}{section.2.3}% 36
\BOOKMARK [2][]{subsection.2.3.4}{2.3.4 End-to-end models}{section.2.3}% 37
\BOOKMARK [2][]{subsection.2.3.5}{2.3.5 Data augmentation}{section.2.3}% 38
\BOOKMARK [3][]{subsubsection.2.3.5.1}{2.3.5.A Adding external data}{subsection.2.3.5}% 39
\BOOKMARK [3][]{subsubsection.2.3.5.2}{2.3.5.B Modifying available data}{subsection.2.3.5}% 40
\BOOKMARK [2][]{subsection.2.3.6}{2.3.6 Training procedure for children speech recognition}{section.2.3}% 41
\BOOKMARK [3][]{subsubsection.2.3.6.1}{2.3.6.A Transfer learning}{subsection.2.3.6}% 42
\BOOKMARK [3][]{subsubsection.2.3.6.2}{2.3.6.B Multi-task learning}{subsection.2.3.6}% 43
\BOOKMARK [3][]{subsubsection.2.3.6.3}{2.3.6.C Self-supervised Learning}{subsection.2.3.6}% 44
\BOOKMARK [1][]{section.2.4}{2.4 Children Corpora}{chapter.2}% 45
\BOOKMARK [2][]{subsection.2.4.1}{2.4.1 LETSREAD}{section.2.4}% 46
\BOOKMARK [2][]{subsection.2.4.2}{2.4.2 PFSTAR\137SWEDISH}{section.2.4}% 47
\BOOKMARK [2][]{subsection.2.4.3}{2.4.3 ETLTDE}{section.2.4}% 48
\BOOKMARK [2][]{subsection.2.4.4}{2.4.4 CMU\137KIDS}{section.2.4}% 49
\BOOKMARK [2][]{subsection.2.4.5}{2.4.5 CHOREC}{section.2.4}% 50
\BOOKMARK [2][]{subsection.2.4.6}{2.4.6 MyST}{section.2.4}% 51
\BOOKMARK [1][]{section.2.5}{2.5 Summary}{chapter.2}% 52
\BOOKMARK [0][]{chapter.3}{3 Hybrid models for children automatic speech recognition}{}% 53
\BOOKMARK [1][]{section.3.1}{3.1 Factorised Time Delay Neural Network for children ASR}{chapter.3}% 54
\BOOKMARK [1][]{section.3.2}{3.2 Assessing the efficacy of multi-task and transfer learning from adult to children}{chapter.3}% 55
\BOOKMARK [2][]{subsection.3.2.1}{3.2.1 Methodology}{section.3.2}% 56
\BOOKMARK [2][]{subsection.3.2.2}{3.2.2 Corpus}{section.3.2}% 57
\BOOKMARK [2][]{subsection.3.2.3}{3.2.3 Experimental setup}{section.3.2}% 58
\BOOKMARK [2][]{subsection.3.2.4}{3.2.4 Results}{section.3.2}% 59
\BOOKMARK [1][]{section.3.3}{3.3 Combining multi-task and transfer learning using multilingual children data}{chapter.3}% 60
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Motivation}{section.3.3}% 61
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 The Multilingual-transfer learning approach}{section.3.3}% 62
\BOOKMARK [2][]{subsection.3.3.3}{3.3.3 Experimental Setup}{section.3.3}% 63
\BOOKMARK [2][]{subsection.3.3.4}{3.3.4 Multilingual-transfer learning experiment}{section.3.3}% 64
\BOOKMARK [2][]{subsection.3.3.5}{3.3.5 Cross-lingual validation}{section.3.3}% 65
\BOOKMARK [1][]{section.3.4}{3.4 Summary and discussion}{chapter.3}% 66
\BOOKMARK [0][]{chapter.4}{4 End-to-End children automatic speech recognition}{}% 67
\BOOKMARK [1][]{section.4.1}{4.1 Transformer model}{chapter.4}% 68
\BOOKMARK [1][]{section.4.2}{4.2 Conformer model}{chapter.4}% 69
\BOOKMARK [1][]{section.4.3}{4.3 Exploring transfer learning efficacy for Transformer-based models}{chapter.4}% 70
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 Partial Transfer learning}{section.4.3}% 71
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Experimental setup}{section.4.3}% 72
\BOOKMARK [2][]{subsection.4.3.3}{4.3.3 Corpus}{section.4.3}% 73
\BOOKMARK [2][]{subsection.4.3.4}{4.3.4 Implementation details}{section.4.3}% 74
\BOOKMARK [2][]{subsection.4.3.5}{4.3.5 Encoder-Decoder Transfer learning}{section.4.3}% 75
\BOOKMARK [2][]{subsection.4.3.6}{4.3.6 Modules Transfer learning}{section.4.3}% 76
\BOOKMARK [1][]{section.4.4}{4.4 Summary and discussion}{chapter.4}% 77
\BOOKMARK [0][]{chapter.5}{5 Exploring Parameter-Efficient Strategies in Transfer Learning for Children-Focused ASR Systems}{}% 78
\BOOKMARK [1][]{section.5.1}{5.1 Adapter tuning}{chapter.5}% 79
\BOOKMARK [1][]{section.5.2}{5.2 Investigating Adapters for Children's ASR}{chapter.5}% 80
\BOOKMARK [1][]{section.5.3}{5.3 Implementation details}{chapter.5}% 81
\BOOKMARK [1][]{section.5.4}{5.4 Results}{chapter.5}% 82
\BOOKMARK [2][]{subsection.5.4.1}{5.4.1 Adapter Configurations}{section.5.4}% 83
\BOOKMARK [2][]{subsection.5.4.2}{5.4.2 Effect of the Adapters hidden dimension}{section.5.4}% 84
\BOOKMARK [2][]{subsection.5.4.3}{5.4.3 Unsupervised clustering for grouped-speaker Adapters}{section.5.4}% 85
\BOOKMARK [1][]{section.5.5}{5.5 Summary and discussion}{chapter.5}% 86
\BOOKMARK [0][]{chapter.6}{6 Integration of synthetic speech for data augmentation}{}% 87
\BOOKMARK [1][]{section.6.1}{6.1 Enhancing ASR Performance through TTS Data Augmentation}{chapter.6}% 88
\BOOKMARK [1][]{section.6.2}{6.2 Closing the synthetic and real mismatch gap with Adapters: The Double Way Adapter Tunning approach}{chapter.6}% 89
\BOOKMARK [1][]{section.6.3}{6.3 Overview of the automatic speech recognition and text-to-speech systems}{chapter.6}% 90
\BOOKMARK [2][]{subsection.6.3.1}{6.3.1 Transformer architecture for ASR}{section.6.3}% 91
\BOOKMARK [2][]{subsection.6.3.2}{6.3.2 Multi-speaker text-to-speech: YourTTS}{section.6.3}% 92
\BOOKMARK [1][]{section.6.4}{6.4 Experimental setup}{chapter.6}% 93
\BOOKMARK [2][]{subsection.6.4.1}{6.4.1 Synthetic data}{section.6.4}% 94
\BOOKMARK [2][]{subsection.6.4.2}{6.4.2 Experiments}{section.6.4}% 95
\BOOKMARK [1][]{section.6.5}{6.5 Results and discussion}{chapter.6}% 96
\BOOKMARK [2][]{subsection.6.5.1}{6.5.1 Comparison with existing approaches}{section.6.5}% 97
\BOOKMARK [2][]{subsection.6.5.2}{6.5.2 Influence of synthetic number of hours}{section.6.5}% 98
\BOOKMARK [2][]{subsection.6.5.3}{6.5.3 Impact of DWAT different hyperparameter configurations}{section.6.5}% 99
\BOOKMARK [2][]{subsection.6.5.4}{6.5.4 Extension of DWAT to the Conformer architecture}{section.6.5}% 100
\BOOKMARK [1][]{section.6.6}{6.6 Summary and discussion}{chapter.6}% 101
\BOOKMARK [0][]{chapter.7}{7 Enhanced parameter-efficient transfer learning with Shared-Adapters}{}% 102
\BOOKMARK [1][]{section.7.1}{7.1 Exploring PETL literature alternatives}{chapter.7}% 103
\BOOKMARK [2][]{subsection.7.1.1}{7.1.1 Scaled Adapters}{section.7.1}% 104
\BOOKMARK [2][]{subsection.7.1.2}{7.1.2 Convolution based Adapters}{section.7.1}% 105
\BOOKMARK [2][]{subsection.7.1.3}{7.1.3 BitFit}{section.7.1}% 106
\BOOKMARK [2][]{subsection.7.1.4}{7.1.4 Scale and Shift features}{section.7.1}% 107
\BOOKMARK [2][]{subsection.7.1.5}{7.1.5 AdapterBias}{section.7.1}% 108
\BOOKMARK [2][]{subsection.7.1.6}{7.1.6 Experimental setup}{section.7.1}% 109
\BOOKMARK [2][]{subsection.7.1.7}{7.1.7 Results of the different PETL methods}{section.7.1}% 110
\BOOKMARK [1][]{section.7.2}{7.2 Leveraging Transformer-based model redundancy for Shared-Adapter}{chapter.7}% 111
\BOOKMARK [2][]{subsection.7.2.1}{7.2.1 Shared-Adapter}{section.7.2}% 112
\BOOKMARK [2][]{subsection.7.2.2}{7.2.2 Experimental setup}{section.7.2}% 113
\BOOKMARK [2][]{subsection.7.2.3}{7.2.3 Results}{section.7.2}% 114
\BOOKMARK [3][]{subsubsection.7.2.3.1}{7.2.3.A Shared-Adapter compared to other PETL methodologies}{subsection.7.2.3}% 115
\BOOKMARK [3][]{subsubsection.7.2.3.2}{7.2.3.B Evaluating the hidden dimension and parameter influence on Shared-Adapter}{subsection.7.2.3}% 116
\BOOKMARK [3][]{subsubsection.7.2.3.3}{7.2.3.C Low resource and extremely low resource scenarios robustness}{subsection.7.2.3}% 117
\BOOKMARK [1][]{section.7.3}{7.3 Summary and discussion}{chapter.7}% 118
\BOOKMARK [0][]{chapter.8}{8 Conclusions}{}% 119
\BOOKMARK [1][]{section.8.1}{8.1 Summary}{chapter.8}% 120
\BOOKMARK [1][]{section.8.2}{8.2 Perspectives}{chapter.8}% 121
\BOOKMARK [0][]{bib.0}{Bibliography}{}% 122
\BOOKMARK [0][]{section*.137}{Bibliography}{}% 123
\BOOKMARK [1][]{appendix.1}{Appendix A}{section*.137}% 124
\BOOKMARK [0][]{appendix.A}{A Pathological speech detection through pre-trained models}{}% 125
\BOOKMARK [1][]{section.A.1}{A.1 Pathological speech detection using x-vector embeddings}{appendix.A}% 126
\BOOKMARK [2][]{subsection.A.1.1}{A.1.1 Introduction}{section.A.1}% 127
\BOOKMARK [2][]{subsection.A.1.2}{A.1.2 Speaker embeddings: i-vector and x-vector}{section.A.1}% 128
\BOOKMARK [2][]{subsection.A.1.3}{A.1.3 Experimental setup}{section.A.1}% 129
\BOOKMARK [3][]{subsubsection.A.1.3.1}{A.1.3.A Corpora}{subsection.A.1.3}% 130
\BOOKMARK [3][]{subsubsection.A.1.3.2}{A.1.3.B Knowledge based features}{subsection.A.1.3}% 131
\BOOKMARK [3][]{subsubsection.A.1.3.3}{A.1.3.C Speaker embeddings}{subsection.A.1.3}% 132
\BOOKMARK [2][]{subsection.A.1.4}{A.1.4 Results}{section.A.1}% 133
\BOOKMARK [1][]{section.A.2}{A.2 The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge}{appendix.A}% 134
\BOOKMARK [2][]{subsection.A.2.1}{A.2.1 Introduction}{section.A.2}% 135
\BOOKMARK [2][]{subsection.A.2.2}{A.2.2 Corpus}{section.A.2}% 136
\BOOKMARK [2][]{subsection.A.2.3}{A.2.3 Proposed system}{section.A.2}% 137
\BOOKMARK [3][]{subsubsection.A.2.3.1}{A.2.3.A Acoustics modality}{subsection.A.2.3}% 138
\BOOKMARK [3][]{subsubsection.A.2.3.2}{A.2.3.B Linguistic modality}{subsection.A.2.3}% 139
\BOOKMARK [2][]{subsection.A.2.4}{A.2.4 Results}{section.A.2}% 140
\BOOKMARK [1][]{section.A.3}{A.3 Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19}{appendix.A}% 141
\BOOKMARK [2][]{subsection.A.3.1}{A.3.1 Introduction}{section.A.3}% 142
\BOOKMARK [2][]{subsection.A.3.2}{A.3.2 Corpora}{section.A.3}% 143
\BOOKMARK [2][]{subsection.A.3.3}{A.3.3 Proposed system}{section.A.3}% 144
\BOOKMARK [3][]{subsubsection.A.3.3.1}{A.3.3.A TDNN-F embedddings}{subsection.A.3.3}% 145
\BOOKMARK [3][]{subsubsection.A.3.3.2}{A.3.3.B CNN embedddings}{subsection.A.3.3}% 146
\BOOKMARK [3][]{subsubsection.A.3.3.3}{A.3.3.C PASE+ embedddings}{subsection.A.3.3}% 147
\BOOKMARK [3][]{subsubsection.A.3.3.4}{A.3.3.D COVID-19 condition classification}{subsection.A.3.3}% 148
\BOOKMARK [2][]{subsection.A.3.4}{A.3.4 Results}{section.A.3}% 149
\BOOKMARK [1][]{section.A.4}{A.4 Conclusion and future work}{appendix.A}% 150
\BOOKMARK [1][]{appendix.1}{Appendix B}{appendix.A}% 151
\BOOKMARK [0][]{appendix.B}{B Sefl-supervised learning models as feature extractor for children's ASR}{}% 152
\BOOKMARK [1][]{section.B.1}{B.1 Self-supervised pre-trained models}{appendix.B}% 153
\BOOKMARK [2][]{subsection.B.1.1}{B.1.1 Generative modeling}{section.B.1}% 154
\BOOKMARK [2][]{subsection.B.1.2}{B.1.2 Discriminative modeling}{section.B.1}% 155
\BOOKMARK [1][]{section.B.2}{B.2 Experimental setup}{appendix.B}% 156
\BOOKMARK [1][]{section.B.3}{B.3 Results}{appendix.B}% 157
\BOOKMARK [1][]{section.B.4}{B.4 Analysis of the extracted features}{appendix.B}% 158
\BOOKMARK [1][]{section.B.5}{B.5 Conclusions and future work}{appendix.B}% 159
