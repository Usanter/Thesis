{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.1}{\ignorespaces Architecture of the standard Transformer \cite {vaswani2017attention}. a) scaled dot-product attention, b) multi-head self-attention, c) Transformer-Encoder, d) Transformer-Decoder.\relax }}{\reset@font\mlffont 70}{figure.caption.90}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.2}{\ignorespaces Architecture of a Conformer layer\relax }}{\reset@font\mlffont 73}{figure.caption.95}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.3}{\ignorespaces Convolution module in the context of a Conformer layer\relax }}{\reset@font\mlffont 73}{figure.caption.96}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.4}{\ignorespaces Layers-wise up-way and down-way transfer learning experiment for Transformer and Conformer architecture\relax }}{\reset@font\mlffont 83}{figure.caption.100}}
{\reset@font\mlfSfont\mtc@string\contentsline{subfigure}{\noexpand \leavevmode \numberline {(a)}{\ignorespaces {Results of the transfer learning layer-wise for the Transformer model}}}{\reset@font\mlfSfont 83}{figure.caption.100}}
{\reset@font\mlfSfont\mtc@string\contentsline{subfigure}{\noexpand \leavevmode \numberline {(b)}{\ignorespaces {Results of the transfer learning layer-wise for the Conformer model}}}{\reset@font\mlfSfont 83}{figure.caption.100}}
