{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.1}{\ignorespaces Architecture of the standard Transformer \cite {vaswani2017attention}. a) scaled dot-product attention, b) multi-head self-attention, c) Transformer-encoder, d) Transformer-decoder.\relax }}{\reset@font\mlffont 68}{figure.caption.24}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.2}{\ignorespaces Architecture of a Conformer layer\relax }}{\reset@font\mlffont 70}{figure.caption.25}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.3}{\ignorespaces Convolution module in the context of a conformer layer\relax }}{\reset@font\mlffont 71}{figure.caption.26}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {4.4}{\ignorespaces Layers-wise up-way and down-way transfer learning experiment for Transformer and Conformer architecture\relax }}{\reset@font\mlffont 75}{figure.caption.29}}
{\reset@font\mlfSfont\mtc@string\contentsline{subfigure}{\noexpand \leavevmode \numberline {(a)}{\ignorespaces {Results of the transfer learning layer wise for the Transformer model}}}{\reset@font\mlfSfont 75}{figure.caption.29}}
{\reset@font\mlfSfont\mtc@string\contentsline{subfigure}{\noexpand \leavevmode \numberline {(b)}{\ignorespaces {Results of the transfer learning layer wise for the Conformer model}}}{\reset@font\mlfSfont 75}{figure.caption.29}}
