\babel@toc {english}{}
\addvspace {10\p@ }
\contentsline {xchapter}{Introduction}{1}{chapter.1}
\addvspace {10\p@ }
\contentsline {xchapter}{Background - Children automatic speech recognition}{11}{chapter.2}
\contentsline {table}{\numberline {2.1}{\ignorespaces Non-exhaustive comparison of children's speech corpora. This table has been sorted by age range. Blanks indicate unavailable information. Entries highlighted in bold correspond to the corpora used in the experiments presented in this thesis. K: Kindergarden. G: Grade\relax }}{49}{table.caption.73}
\addvspace {10\p@ }
\contentsline {xchapter}{Hybrid models for children automatic speech recognition}{53}{chapter.3}
\contentsline {table}{\numberline {3.1}{\ignorespaces Number of utterances, number of speakers, and the duration of training and testing sets for both English and Portuguese corpora, encompassing both adult and children training and test sets\relax }}{58}{table.caption.78}
\contentsline {table}{\numberline {3.2}{\ignorespaces WER results using adult data for knowledge transfer methods\relax }}{60}{table.caption.83}
\contentsline {table}{\numberline {3.3}{\ignorespaces Statistics on the different corpora of children's speech.\relax }}{65}{table.caption.85}
\contentsline {table}{\numberline {3.4}{\ignorespaces WER results of multilingual-transfer learning and cross-lingual experiments. MTL: Multi-Task Learning, TL: Transfer Learning, MLTL: Multilingual Transfer Learning, MLTL-olo: Multilingual Transfer Learning one-language-out\relax }}{66}{table.caption.86}
\addvspace {10\p@ }
\contentsline {xchapter}{End-to-End children automatic speech recognition}{71}{chapter.4}
\contentsline {table}{\numberline {4.1}{\ignorespaces My Science Tutor Children Speech Corpus statistics\relax }}{81}{table.caption.97}
\contentsline {table}{\numberline {4.2}{\ignorespaces Encoder-Decoder experiment\relax }}{82}{table.caption.98}
\contentsline {table}{\numberline {4.3}{\ignorespaces Modules fine-tuning experiment\relax }}{84}{table.caption.100}
\addvspace {10\p@ }
\contentsline {xchapter}{Exploring Parameter-Efficient Strategies in Transfer Learning for Children-Focused ASR Systems}{89}{chapter.5}
\contentsline {table}{\numberline {5.1}{\ignorespaces Results of the different Adapters configurations in both Transformer and Conformer.\relax }}{97}{table.caption.107}
\contentsline {table}{\numberline {5.2}{\ignorespaces Results of the unsupervised clustered Adapters approach.\relax }}{99}{table.caption.109}
\addvspace {10\p@ }
\contentsline {xchapter}{Integration of synthetic speech for data augmentation}{103}{chapter.6}
\contentsline {table}{\numberline {6.1}{\ignorespaces Results of the different use of synthetic data augmentation approaches in WER. Collomn TTS$_1$ and TTS$_2$ correspond to the respective model used to generate the synthetic data.\relax }}{113}{table.caption.117}
\contentsline {table}{\numberline {6.2}{\ignorespaces Results of the different number of hours influence in our DWAT approach with \textit {Large Synth$_2$} data\relax }}{114}{table.caption.118}
\contentsline {table}{\numberline {6.3}{\ignorespaces Results of the different configurations of Adapter double-way approach on 300h of \textit {Synth$_2$}\relax }}{115}{table.caption.119}
\contentsline {table}{\numberline {6.4}{\ignorespaces Scores for the different methods of synthetic data augmentation within the Conformer architecture\relax }}{117}{table.caption.121}
\addvspace {10\p@ }
\contentsline {xchapter}{Alternative approaches to parameter-efficient transfer learning}{121}{chapter.7}
\contentsline {table}{\numberline {7.1}{\ignorespaces Performances of the different PETL alternatives on children ASR.\relax }}{129}{table.caption.127}
\contentsline {table}{\numberline {7.2}{\ignorespaces WER and Parameters for different Shared-Adapter hidden dimension\relax }}{134}{table.caption.130}
\contentsline {table}{\numberline {7.3}{\ignorespaces WER for different training durations for the full model fine-tune, TPA Adapter, and Shared-Adapter\relax }}{134}{table.caption.131}
\addvspace {10\p@ }
\contentsline {xchapter}{Conclusions}{137}{chapter.8}
\addvspace {10\p@ }
\contentsline {xchapter}{Pathological speech detection through pre-trained models}{175}{appendix.A}
\contentsline {table}{\numberline {A.1}{\ignorespaces Description of Speakers and Segments\relax }}{178}{table.caption.142}
\contentsline {table}{\numberline {A.2}{\ignorespaces X-vector network Description\relax }}{179}{table.caption.143}
\contentsline {table}{\numberline {A.3}{\ignorespaces Results of the different tasks with KB and speaker embeddings\relax }}{179}{table.caption.145}
\contentsline {table}{\numberline {A.4}{\ignorespaces Statistical information on the ADReSS corpus\relax }}{181}{table.caption.147}
\contentsline {table}{\numberline {A.5}{\ignorespaces Results of different acoustic approaches on the development set\relax }}{183}{table.caption.149}
\contentsline {table}{\numberline {A.6}{\ignorespaces Results of different linguistic approaches on the development set\relax }}{183}{table.caption.150}
\contentsline {table}{\numberline {A.7}{\ignorespaces Results of different acoustic and linguistic approaches on the test set\relax }}{184}{table.caption.151}
\contentsline {table}{\numberline {A.8}{\ignorespaces Performance results (unweighted average recall-UAR) on the COVID-19 COUGH (C19C) corpus\relax }}{188}{table.caption.153}
\addvspace {10\p@ }
\contentsline {xchapter}{Sefl-supervised learning as feature extractor for children's \ac {ASR}}{191}{appendix.B}
\contentsline {table}{\numberline {B.1}{\ignorespaces Overview of different SSL architectures used as frozen feature extractors\relax }}{192}{table.caption.155}
\contentsline {table}{\numberline {B.2}{\ignorespaces My Science Tutor Children Speech Subset Corpus statistics\relax }}{194}{table.caption.157}
\contentsline {table}{\numberline {B.3}{\ignorespaces Results without language model of different Self-supervised models as feature extractors\relax }}{195}{table.caption.158}
