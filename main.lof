\babel@toc {english}{}
\addvspace {10\p@ }
\contentsline {xchapter}{Introduction}{1}{chapter.1}
\contentsline {figure}{\numberline {1.1}{\ignorespaces Illustrated herein are some examples of children's Speech and Language Technology applications that were developed during the course of this thesis. On the left is a running platformer game, where the user's voice controls the character. Pitch dictates running and jumping actions, while energy modulates the velocity of these actions. On the right, a reading task game is depicted, wherein a robot instructs the user to read designated words.\relax }}{5}{figure.caption.5}
\addvspace {10\p@ }
\contentsline {xchapter}{Background - Children automatic speech recognition}{9}{chapter.2}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Formant and cepstral variability. Figures taken from \cite {reviewASRchildren}\relax }}{14}{figure.caption.6}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Changes in F1-F2 vowel space as a function of age}}}{14}{figure.caption.6}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Mean cepstral distance between the two repetitions of the same vowels}}}{14}{figure.caption.6}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Segmental duration variability. Figures taken from \cite {Acoustic_change_children}\relax }}{15}{figure.caption.7}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Averaged-vowel duration across all vowels and subjects in each age group}}}{15}{figure.caption.7}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Within- and between-subject variations. The between-subject variation is reduced by a factor of 2.0}}}{15}{figure.caption.7}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a standard digit pattern from Davis et al. 1952\relax }}{20}{figure.caption.8}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of a decoding graph from the Harpy system for the sentence "GIVE ME" from \cite {klatt1977review}\relax }}{22}{figure.caption.9}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Architecture of a HMM-based speech recognition system\relax }}{23}{figure.caption.10}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Principal block scheme of main speech features for ASR: Melspec, fbanks and MFCC coefficients from \cite {kiktova2013comparison}\relax }}{25}{figure.caption.11}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Three-state Hidden Markov Model for modelling phones\relax }}{26}{figure.caption.12}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Phoneme set and examples of CMU dictionary using 39 phonemes from \cite {weide1998carnegie}\relax }}{28}{figure.caption.13}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Architecture of an end-to-end speech recognition system\relax }}{31}{figure.caption.14}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Transfer learning approaches. Figures from \cite {TFchildren}\relax }}{42}{figure.caption.15}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Acoustic adaptation}}}{42}{figure.caption.15}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Pronunciation adaptation}}}{42}{figure.caption.15}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Multilingual approach using each language as a task in a multi-task learning context.\relax }}{44}{figure.caption.16}
\addvspace {10\p@ }
\contentsline {xchapter}{Hybrid models for children automatic speech recognition}{51}{chapter.3}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Multilingual transfer learning approach. Language-specific layers can be randomly initialized for a language not present during the MTL phase or use the corresponding pre-trained layers in case the target language was present during the MTL phase. Grey blocks are pre-trained during MTL phase.\relax }}{58}{figure.caption.21}
\addvspace {10\p@ }
\contentsline {xchapter}{End-to-End children automatic speech recognition}{63}{chapter.4}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Architecture of the standard Transformer \cite {vaswani2017attention}. a) scaled dot-product attention, b) multi-head self-attention, c) Transformer-encoder, d) Transformer-decoder.\relax }}{66}{figure.caption.24}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Architecture of a Conformer layer\relax }}{68}{figure.caption.25}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Convolution module in the context of a conformer layer\relax }}{69}{figure.caption.26}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Up-way and down-way transfer learning experiment\relax }}{74}{figure.caption.29}
\addvspace {10\p@ }
\contentsline {xchapter}{Exploring parameters-efficient transfer learning for end-to-end children ASR}{77}{chapter.5}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Residual Adapter architecture\relax }}{80}{figure.caption.31}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Conformer block with various residual adapter configurations. Normalisation layers are not display in this figure\relax }}{82}{figure.caption.32}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Conformer block with various residual adapter configurations. Normalisation layers are not display in this figure\relax }}{82}{figure.caption.33}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Different paramter efficent procedure for children ASR in conformer model\relax }}{87}{figure.caption.36}
\contentsline {figure}{\numberline {5.5}{\ignorespaces AdapterBias, consisting of a linear layer $L_\alpha $ and a vector $\mathcal {V}$, is added after the second feed-forward layer only in each FFN module.\relax }}{90}{figure.caption.37}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Different paramter efficent procedure for children ASR in conformer model with shared-Adapters\relax }}{91}{figure.caption.38}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Shared-Adapter setup in a Conformer model\relax }}{92}{figure.caption.39}
\addvspace {10\p@ }
\contentsline {xchapter}{Use of synthetic speech as data augmentation}{93}{chapter.6}
\contentsline {figure}{\numberline {6.1}{\ignorespaces Overview of a) double way fine-tuning and b) Adapter layer architecture\relax }}{97}{figure.caption.41}
\addvspace {10\p@ }
\contentsline {xchapter}{Pathology detection from speech}{107}{chapter.7}
