% #############################################################################
% This is Chapter 8
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Conclusions}
\label{chap:8}
\cleardoublepage

% Shift and motivation
The primary objective of this thesis was to improve \ac{ASR} for children's speech. Initially, the focus was on the necessity of developing a reliable foundational children's \ac{ASR} model to support the development of speech and language technologies for children with pathological speech. However, the focus of the thesis shifted towards solely on improving healthy children's \ac{ASR}. This redirection expanded the potential applications of our research, moving beyond the initial scope of possible applications in speech and language technologies. New potential applications include personal assistants, automatic reading tutors, and voice interactions with computer interfaces. However, we note that recognising the unique characteristics and challenges of children's \ac{ASR} is crucial for developing a foundational \ac{ASR} model that is robust across different contexts, including pathological children's speech.

% Contributions
This thesis has made significant contributions to the field of children's \ac{ASR}, with a particular emphasis on improving traditional knowledge transfer methods which represented the state-of-the-art methods for children's \ac{ASR}. The proposed improvements in this thesis aims to make these models more robust, granular, and parameter-efficient, both in hybrid and end-to-end frameworks. In this concluding chapter, we provide a comprehensive summary of the various works developed during this thesis, highlighting key discoveries and contributions to the field of children's \ac{ASR}.

\section{Summary of the work carried out during the thesis}
% SOTA
In the first part of the thesis, we establish the context for our research and outline the challenges associated with children's speech recognition. The primary challenge arises from the high variability exhibited in children's speech. Notable features of children's speech include frequency ranges that are shifted and broadened compared to adult speech, along with significant inter- and intra-speaker variability. Furthermore, the process of language acquisition for young children adds an additional layer of complexity, making it challenging for both human listeners and automated systems to accurately recognise children's speech. Addressing this complexity necessitates a substantial amount of data, constituting a second major challenge due to the scarcity of available children's speech data. In this initial section, we present a non-exhaustive list of children's speech corpora available in the literature, representing the most extensive compilation to the best of our knowledge. Additionally, we delve into various approaches employed in the literature to tackle the diverse challenges inherent in children's \ac{ASR}.

% HMM
In the second part of the thesis, our focus was on the development and exploration of a hybrid \ac{HMM-DNN} \ac{ASR} system, specifically designed for low-resource children's \ac{ASR}, with a particular emphasis on the European Portuguese language. Our efforts centered around the evaluation of various knowledge transfer approaches, with a particular focus on their efficiency. Among the different approaches considered, transfer learning emerged as the most effective for systems dedicated solely to recognising children's speech. Additionally, multi-task learning proved effective when the system needed to recognise both children and adult speech simultaneously.

% Transfer learning E2E
In the subsequent part of our research, we transitioned towards the end-to-end paradigm, seeking to enhance the current state-of-the-art approaches. Instead of transfer learning over the entire model, we proposed a more granular evaluation. Our investigation revealed that the Encoder plays a pivotal role in the fine-tuning process for end-to-end children's \ac{ASR}. This aligns with the understanding that in the context of children's speech, acoustic variability highly contribute in the degradation of recognition accuracy, more than linguistic factors. Furthermore, our findings highlighted that higher layers, closer to the output of the Encoder, yield more substantial benefits in fine-tuning compared to lower layers, near the input of the Encoder. These insights offer valuable recommendations for the development of children's \ac{ASR} models through transfer learning. In this section of the thesis, we also introduced the partial fine-tuning approach for Transformer-based architectures. We identified that fine-tuning only specific components of the network outperformed entire model fine-tuning, with the \ac{FFN} component being recognised as the most crucial one with a relative \ac{WER} improvement around 9\% for both Transformer and Conformer architectures.

% Adapters
Next, motivated by the need of parameter efficiency knowledge transfer, especially in scenarios with limited training data, we delved into the exploration of the use Adapter modules. Adapter modules, comprising two linear layers integrated into a pre-trained frozen model, serve as a mechanism for knowledge transfer while preserving the weights and knowledge contained in the pre-trained model. We evaluated various configurations within both Transformer and Conformer architectures. Among all configurations tested, the parallel configuration, and its Conformer extension known as TPA, emerged as the most effective for transferring knowledge to children's speech. Remarkably, these configurations outperformed entire model fine-tuning by achieving better results while using only 10\% of the number of parameters involved in entire model fine-tuning. This evaluation demonstrated the viability of Adapters in the context of children's \ac{ASR}, suggesting their applicability for more precise adaptation. For this purpose, we introduced an unsupervised procedure in which utterances were clustered using k-means applied to their respective x-vectors. The rationale behind this approach lies in the notion that utterances sharing acoustic similarities, as captured by the x-vector extractor, would benefit more from an Adapter specifically trained on similar utterances rather than a general children's Adapter. Our experiments, involving the manipulation of cluster numbers, yielded additional improvements in children's speech recognition results.

% TTS
Expanding on the efficiency of Adapters in bridging the gap between the source and target domains for children's speech, we employed Adpaters for better data augmentation with imperfect data for children's \ac{ASR}. Specifically, we introduced the \ac{DWAT}, incorporating \ac{TTS} data. Given that synthetic speech often exhibits acoustics mismatch with real speech, there is a crucial need to reduce this gap, which our \ac{DWAT} approach addressed. This two-step procedure involved the initial training of Adapter modules using imperfect \ac{TTS} data, followed by the fine-tuning of Adapters and the entire model weights using both synthetic and real data. The data underwent a distinctive dual-pathway, with synthetic speech passing through the Adapters while real speech skipped this pathway. The \ac{DWAT} approach resulted in notable improvements over baseline and previous techniques across both Transformer and Conformer architectures, showcasing the efficacy of our approach. Additionally, we extended speaker embedding filtering by incorporating x-vectors instead of i-vectors, using cosine similarity between the reference and generated utterances to discard weak similarities that may represent incorrectly generated utterances.

% Shared-Adapters
Finally, motivated by the different successes of Adapters, both in Adapter transfer and the \ac{DWAT} approach, we evaluated different alternatives present in the literature. Our observations for children's \ac{ASR} revealed that, among the various parameter-efficient methods, traditional Adapters remained the most effective. We noted a tradeoff between accuracy and parameter efficiency â€“ some methods were highly parameter-efficient but led to significantly degraded results, while others were less parameter-efficient but yielded comparable or better results than entire model fine-tuning. To address this tradeoff, we proposed a novel approach that leverages the high redundancy present in the \ac{FFN} components of Transformer-based models. Our Shared-Adapters, where only one Adapter is shared across all layers instead of having one per layer, outperformed the entire model fine-tuning while facing a minimal degradation compared to traditional Adapter, which was significantly smaller than any previous approach. Using less than 1\% of the entire model's number of parameters compared to the traditional Adapter's 10\%, the Shared-Adapters provide a promising solution to the tradeoff challenge and emerge as an excellent candidate for achieving better parameter efficiency transfer in children's \ac{ASR}.

\section{Perspectives}
At the conclusion of this thesis, several promising avenues for further improvement of children's \ac{ASR} have been identified. These perspectives could provide a foundation for future research endeavors in the field of children's \ac{ASR}, aiming to advance our understanding and enhance the capabilities of automated speech recognition systems tailored for young speakers. These perspectives include:

% DWAT for other domains
\paragraph*{Extension of the Double-way Adapter Transfer to other domains} The DWAT approach has proven to be effective in enhancing the overall fine-tuning process by using Adapters to bridge the gap between two distinct domains, particularly between synthetic and real data. An interesting avenue for further research would be to extend this approach to novel domains, such as adult and children's speech, exploring the adaptability and efficacy of Adapters in diverse contexts. Furthermore, in our work, we primarily employed entire model fine-tuning; however, our findings of chapter \ref{chap:4} suggest that a more granular fine-tuning approach could yield even better results. This indicates the importance of investigating the fine-tuning process of the DWAT approach at a more detailed level, focusing on specific components or layers rather than the entire network. 

% Shared Adapter
\paragraph*{Exploration of novel non redundant Transformer-based architecture for efficient fine-tuning} The sucess of our Shared-Adapter shed the light on the redundancy amount the \ac{FFN} components of the differents layers of Transformer-based models. As we showed during this thesis, fine-tuning  overparameterised network with small amount of data could lead to overfitting or decreased \ac{WER} scores. Therefore, it would be interesting to develop a novel architecture of Transformer-based \ac{ASR} models that would not be as redundant. Either by using a shared FNN component in a similar way as \cite{pires2023one}. This model would be easier to fine-tuned. In addition, a mixed Shared and layers wise \ac{FFN} modules could be as well an intereting avenue of research.

\paragraph*{Furhter exploration of Shared Adapters} The good results observed with Shared-Adapters paves the way for further developments, suggesting potential exploration of hybrid approaches that combine shared Adapters with layer-wise Adapters. This mixed approach could offer a more nuanced strategy for parameter-efficient transfer learning in children's \ac{ASR}. Exploring the synergies between different Adapter configurations could contribute to refining the adaptability of models to children's speech while maintaining high efficiency.

% Developement of SLT applications
\paragraph*{Developement of Speech and Language Technology based on improved children's Automatic Speech Recognition} Collecting pathological speech data for children in American English is a valuable and important initiative. The availability of such data will contribute to the development of technology that is specifically tailored for children with speech disorders using our improved children's \ac{ASR} systems. This can lead to improved speech and language technologies, allowing for more inclusive and effective solutions in the context of children's \ac{ASR}. 