{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.1}{\ignorespaces Residual Adapter architecture\relax }}{\reset@font\mlffont 80}{figure.caption.31}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.2}{\ignorespaces Transformer block with various residual adapter configurations. Normalisation layers are not display in this figure\relax }}{\reset@font\mlffont 82}{figure.caption.32}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.3}{\ignorespaces Conformer block with various residual adapter configurations. Normalisation layers are not display in this figure\relax }}{\reset@font\mlffont 82}{figure.caption.33}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.4}{\ignorespaces Th architecture of the ConvPass adapter. $k$ is the kernel size of the 1D convolution. All Convoluation are depth-wise convolution.\relax }}{\reset@font\mlffont 89}{figure.caption.36}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.5}{\ignorespaces AdapterBias, consisting of a linear layer $L_\alpha $ and a vector $\mathcal {V}$, is added after the second feed-forward layer only in each FFN module.\relax }}{\reset@font\mlffont 91}{figure.caption.37}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.6}{\ignorespaces Different paramter efficent procedure for children ASR in conformer model\relax }}{\reset@font\mlffont 92}{figure.caption.38}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.7}{\ignorespaces Different paramter efficent procedure for children ASR in conformer model with shared-Adapters\relax }}{\reset@font\mlffont 95}{figure.caption.40}}
{\reset@font\mlffont\mtc@string\contentsline{figure}{\noexpand \leavevmode \numberline {5.8}{\ignorespaces Shared-Adapter setup in a Conformer model\relax }}{\reset@font\mlffont 96}{figure.caption.41}}
